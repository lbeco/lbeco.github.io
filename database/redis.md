

# redis

## redis持久化

aof：append only file 持久化的日志文件

rdb：redis文件的快照

redis推荐同时使用这二者进行持久化

以bg形式fork一个线程来生成

aof写在buffer里面，不过会做定时的刷盘处理。刷盘周期可以配置。

### 主从复制

https://blog.csdn.net/meser88/article/details/122292238

在Redis2.8以前，从节点向主节点发送sync命令请求同步数据，此时的同步方式是全量复制；在Redis2.8及以后，从节点可以发送psync命令请求同步数据，此时根据主从节点当前状态的不同，同步方式可能是全量复制或增量复制。

2.8以后：

全量复制：

- 从节点判断无法进行增量复制，向主节点发送全量复制的请求；或从节点发送增量复制的请求，但主节点判断无法进行全量复制。
- 主节点收到全量复制的命令后，执行bgsave，在后台生成RDB文件，并使用一个缓冲区（称为复制缓冲区）记录从现在开始执行的所有写命令。
- 主节点的bgsave执行完成后，将RDB文件发送给从节点；从节点首先清除自己的旧数据，然后载入接收的RDB文件，将数据库状态更新至主节点执行bgsave时的数据库状态。
- 主节点将前述复制缓冲区中的所有写命令发送给从节点，从节点执行这些写命令，将数据库状态更新至主节点的最新状态。
- 如果从节点开启了AOF，则会触发bgrewriteaof的执行，从而保证AOF文件更新至主节点的最新状态。

增量复制：

从机连接主机后，会主动发起 PSYNC 命令，从机会提供 master 的 runid(机器标识，随机生成的一个串) 和 offset（数据偏移量，如果offset主从不一致则说明数据不同步），主机验证 runid 和 offset 是否有效，runid 相当于主机身份验证码，用来验证从机上一次连接的主机，如果 runid 验证未通过则，则进行全同步，如果验证通过则说明曾经同步过，**根据 offset 同步部分数据**。

- 如果offset偏移量之后的数据，仍然都在复制积压缓冲区里，则执行增量复制；
- 如果offset偏移量之后的数据已不在复制积压缓冲区中（数据已被挤出），则执行全量复制。



## 数据类型：

https://www.redis.net.cn/order/  操作

![image-20220607233053204](D:\study\lbeco\lbeco.github.io\database\redis.assets\image-20220607233053204.png)

### **string** 		

key value存储string



#### 操作

​	set key value 设置指定key的value

​	get key 获取指定key的value

#### 底层结构

|  编码  |                    使用场景                     |
| :----: | :---------------------------------------------: |
|  int   |                    整数类型                     |
| embstr | 较小的值使用 3.2后小于44，和redisObject放在一起 |
|  raw   |               较大的值使用 大于44               |

Redis 的 string 类型底层使用的是 SDS(动态字符串) 实现的， 具体数据结构如下：

```c
struct sdshdr {
    int len;        // 记录字符串长度
    int free;       // 记录 buf 数组中未使用字节的数量
    char buf[];     // 保存字符串的字节数组
}
```

优点：可以直接获得长度，保证二进制安全

### **hash**			

hash 是一个 string 类型的 field（字段） 和 value（值） 的映射表。

hset key field value [field value]:  存放指定key的field和value值

#### 操作



#### 底层结构

|   编码    |      使用场景       |
| :-------: | :-----------------: |
|  ziplist  |                     |
| hashtable | 和hashmap类似的结构 |

ziplist 是一个特殊双向链表，不像普通的链表使用前后指针关联在一起，它是存储在连续内存上的。整体的结构布局如下图：

![在这里插入图片描述](D:\study\lbeco\lbeco.github.io\database\redis.assets\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzUxNTA0NTQ1,size_16,color_FFFFFF,t_70#pic_center.png)

五个结构：

- zlbytes: 32 位无符号整型，记录 ziplist 整个结构体的占用空间大小。当然了也包括 zlbytes 本身。这个结构有个很大的用处，就是当需要修改 ziplist 时候不需要遍历即可知道其本身的大小。 这个 SDS 中记录字符串的长度有相似之处，这些好的设计往往在平时的开发中可以采纳一下。
- zltail: 32 位无符号整型, 记录整个 ziplist 中最后一个 entry 的偏移量。所以在尾部进行 POP 操作时候不需要先遍历一次。
- zllen: 16 位无符号整型, 记录 entry 的数量， 所以只能表示 2^16。但是 Redis 作了特殊的处理：当实体数超过 2^16 ,该值被固定为 2^16 - 1。 所以这种时候要知道所有实体的数量就必须要遍历整个结构了。
- entry: 真正存数据的结构。
- zlend: 8 位无符号整型, 固定为 255 。为 ziplist 的结束标识。

每个 entry 都包含两条信息的元数据为前缀。 

- 第一元数据用来存储前一个 entry 的长度，以便能够从后向前遍历列表。
- 第二元数据是表示 entry 的编码形式。 用来表示 entry 类型，整数或字符串，在字符串的情况下，它还表示字符串有效的长度。



渐进式rehash

为了hash不造成系统中止，redis采取了渐进式的rehash策略。

1. 为ht[1]分配空间
2. 将字典中的rehashidx设置为0，表示rehash正式开始，rehash期间，不会阻塞CRUD等操作
3. 当ht[0]所有的键值对都rehash到ht[1]时，将rehashidx属性设置成-1，表示rehash完成

这一过程不会对crud造成影响。

### **list**

链表 可以做栈或者队列 

#### 操作



#### 底层结构

redis list数据结构底层采用压缩列表ziplist或linkedlist两种数据结构进行存储，当创建新的列表键时， 默认以ziplist进行存储，在不满足ziplist的存储要求后转换为linkedlist列表。

**ziplist**

ziplist进行存储时列表的条件：

- 列表对象保存的所有字符串元素的长度小于64字节
- 列表对象保存的元素数量小于512个。

ziplist没有维护双向指针:prev next；而是存储**上一个 entry的长度**和**当前entry的长度**，通过长度推算下一个元素在什么地方。牺牲读取的性能，获得高效的存储空间，因为(简短字符串的情况)存储指针比存储entry长度 更费内存。这是典型的"时间换空间"。

### set			

集合，类似于hash







### zset

zset会把每一个string和一个double绑定，按照double排序。

#### 操作

Zadd [key] [score] [value] 添加元素

Zrange  通过索引区间返回有序集合成指定区间内的成员。当给定 WITHSCORES 选项时,命令会将元素和分值一并返回。

Zrevrange 反向输出

Zrank  返回指定成员的索引



#### 底层结构

在同时满足以下两个条件的时候使用**ziplist**

- 有序集合保存的元素数量小于128个
- 有序集合保存的所有元素的长度小于64字节

每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个元素保存元素的分值。



skiplist**跳表**

 https://blog.csdn.net/weixin_45480785/article/details/116293416

每一次插入时，都会计算层高，有p的概率进入上一层。牺牲了总体的性能换取了速度。

![img](https://img.jbzj.com/file_images/article/202011/202011120920146.png)



### hyperloglog 

基数统计

### bitmap 

操作：

SETBIT、GETBIT、BITCOUNT、BITPOS、BITOP

位图，单个key对应着一个bit

可以用来做布隆过滤器

底层使用的是string

### GEO 

地理位置 计算距离 指定范围内点等 使用geo hash进行编码

事务：需要watch来关注相关数值，watch中数值发生变化就会进行回滚

代码写错了不作回滚

事务没啥人用，太垃圾了

一个名叫过期字典的数据库保存过期时间



## redis删除多余数据

Redis的内存回收机制主要体现在以下两个方面：

- 删除到达过期时间的键对象。
- 内存使用达到`maxmemory`上限时触发内存溢出控制策略。

### 过期删除策略

定时删除 ：在设置某个key 的过期时间同时，我们创建一个定时器，让定时器在该过期时间到来时，立即执行对其进行删除的操作。

惰性删除： 当需要该key时，我们在检查其是否过期，如果过期，我们就删掉它，反之返回该key。

定期删除 ：周期性抽查存储空间 

​	**优点**：可以通过限制删除操作执行的时长和频率来减少删除操作对 CPU 的影响。另外定期删除，也能有效释放过期键占用的内存。

​	**缺点**：难以确定删除操作执行的时长和频率。

* 如果执行的太频繁，定期删除策略变得和定时删除策略一样，对CPU不友好。
* 如果执行的太少，那又和惰性删除一样了，过期键占用的内存不会及时得到释放。
* 另外最重要的是，在获取某个键时，如果某个键的过期时间已经到了，但是还没执行定期删除，那么就会返回这个键的值，这是业务不能忍受的错误

**redis的做法**

Redis所有的键都可以设置过期属性，内部保存在过期字典中。由于进程内保存大量的键，维护每个键精准的过期删除机制会导致消耗大量的CPU，对于单线程的Redis来说成本过高，所以redis同时使用了定期删除和惰性删除

原文链接：https://blog.csdn.net/m0_38017860/article/details/124325088

Redis的惰性删除策略由 `db.c/expireIfNeeded` 函数实现，所有键读写命令执行之前都会调用 `expireIfNeeded` 函数对其进行检查，如果过期，则删除该键，然后执行键不存在的操作；未过期则不作操作，继续执行原有的命令。

定期检查就是执行一个循环，每秒钟执行server.hz次serverCron，循环中的每轮操作会从current_db对应的数据库中随机依次取出w个key，查看其是否过期。如果过期就将其删除， 并且记录删除的key的个数。如果过期的key个数大于25%，就会继续检查当前数据库，当过期的key小于25%，会继续检查下一个数据库。若执行超过了一定时间，则会停止。

### 内存淘汰机制

数据逐出：满了就删除 使用lru/lfu 等算法

几个例子：

1. **volatile-lru（least recently used）**：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰
2. **volatile-ttl**：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰
3. **volatile-random**：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰
4. **allkeys-lru（least recently used）**：当**内存不足**以容纳新写入数据时，在键空间中，移除最近最少使用的 key（**这个是最常用的**）

## redis多机

redis有三种多机模式：

- 单机模式

读和写都由单台服务器来完成，适用于数据量较少的情况

- replication模式，单master，多slave

主从复制，读写分离；master负责写入，slave负责读取，基于单机的基础上，提高了读的效率，但并没有增加存储量或存储效率

- cluster模式，多master，多slave

多master，多slave，适用于数据量特别大的情况；但要注意的是，master负责写和读，slave只是备份master数据，等master挂掉之后，成为新的master，cluster模式并非读写分离。


Sentinel(哨兵)：运行在特定模式下的分布式redis服务器，监控master和slave的运作状态，使用流言协议（gossipprotocols）接收master下线的消息，执行故障转移，使用投票协议推举出新的master，起着监督者的角色。Sentinel**可以一台，也可以多台**。多个Sentinel形成独立的分布式系统。



主观下线：对单个redis节点的心跳没有回复

客观下线：哨兵节点共同判断下线

leader选举：类似raft

假设主服务器宕机，哨兵1先检测到结果，但是系统并不会马上进行failover过程，仅仅是哨兵1主观认为主服务器不可以用，这个现象称为**主观下线**。

哨兵通过gossipprotocols来同步信息。当后面的哨兵也检测到主服务器不可用，并且数量达到一定时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover故障转移操作。
操作转移成功后。就会发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这一过程称为**客观下线**

4.0:混合持久机制

6.0引入多线程

Redis6.0 引入多线程主要是为了提高网络 IO 读写性能，没有改变单线程的操作。

## lua脚本

lua脚本的作用：

- 减少网络开销：本来5次网络请求的操作，可以用一个请求完成，原先5次请求的逻辑放在redis服务器上完成。使用脚本，减少了网络往返时延。
- **原子操作：Redis会将整个脚本作为一个整体执行，中间不会被其他进程或者进程的命令插入**。（最重要）
- 复用：客户端发送的脚本会永久存储在Redis中，意味着其他客户端可以复用这一脚本而不需要使用代码完成同样的逻辑。

使用EVAL命令执行。

问题：没法回滚，机子挂了还是得擦屁股。

### 几大问题

**雪崩**：较短时间内较多key**集中过期**，请求大量积压，导致数据库被打爆，于是redis超时，最后崩溃

解决：预警 页面静态化 多级缓存 限流降级 有效期错峰加随机值 超热数据永久持久化

**击穿**：**单个**key非常热，过期后击穿 

解决：预先设定或现场将过热数据改为永久key，高峰来临时刷新数据有效期；设置二级缓存，保障两个缓存不会同时被淘汰

**穿透**：redis内存正常实命中率随时间下降，数据库压力大，崩掉。原因：出现大量**非正常**url访问，导致去数据库中查找

解决：布隆过滤器，实时监控后黑名单防控 

监控：prometheus cloud insight redis

## redis分片

#### 哈希槽

Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点。

#### 一致性hash

<img src="https://img-blog.csdnimg.cn/0059405a45674e1899e88826c11139f4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-r5oiR5ouW6Z6L5ZOl,size_20,color_FFFFFF,t_70,g_se,x_16" alt="img" style="zoom:33%;" />

简单的说，就是把hash环上的请求分配给顺时针下一个服务器。

**hash偏斜** 多个服务在hash环上靠近，导致请求分配不均匀

所以我们引入了虚拟节点的概念，以A节点为例，虚拟构造出(A0,A1,A2....AN)，只要是落在这些虚拟节点上的数据，都存入A节点。读取时也相同，顺时针获取的是A0虚拟节点，就到A节点上获取数据，这样就能解决数据分布不均的问题。

![img](D:\study\lbeco\lbeco.github.io\database\redis.assets\watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBA5Y-r5oiR5ouW6Z6L5ZOl,size_20,color_FFFFFF,t_70,g_se,x_16-16546208305194.png)



## redis分布式锁

分布式锁：setnx（set if not exist）添加 需要考虑过期时间

示例：set name liuxinglin ex 100 nx

### redisson

redisson是用java实现的redis分布式可重入锁

https://segmentfault.com/a/1190000038988087

防止redis单点故障，引入redlock，redlock可以保证大多数redis服务同意获取的锁，从而实现分布式



### RedLock

redlock主要就是向所有服务器发送锁请求，过半就生效

redlock不需要节点之间的复制，假设有5个redis节点，客户端取锁流程会变成这样：

- 以毫秒为单位获取当前的服务器时间
- 尝试使用相同的key和随机值来获取锁，客户端对每一个机器获取锁时都应该有一个超时时间，比如锁的过期时间为10s，那么获取单个节点锁的超时时间就应该为5到50毫秒左右，这样做的目的是为了保证客户端与故障的机器连接不耗费多余的时间！超时间时间内未获取数据就放弃该节点，从而去下一个Redis节点获取。
- 获取完成后，获取当前时间减去步骤一获取的时间，当且仅当客户端从半数以上(这里是3个节点)的Redis节点获取到锁且获取锁的时间小于锁额超时时间，则证明该锁生效！
- 如果取到了锁，**key的真正有效时间等于有效时间减去获取锁所使用的时间**（步骤3计算的结果）。
- 如果获取锁的机器不满足半数以上，或者锁的超时时间计算完毕后为负数等异常操作，则系统会尝试解锁所有实例，即便某些Redis实例根本就没有加锁成功，防止某些节点获取到锁但是客户端没有得到响应而导致接下来的一段时间不能被重新获取锁

RedLock不能提供可靠的服务





## 缓存一致性

一般我们在更新数据库数据时，需要同步redis中缓存的数据

缓存刷新需要时间长，可能也没啥用，故一般采取缓存删除的策略。

存在两种方法：
（1）第一种方案：先执行update操作，再执行缓存清除。
（2）第二种方案：先执行缓存清除，再执行update操作。

弊端:当存在并发请求时，很容易出现问题
（1）第一种方案：当请求1执行update操作后，还未来得及进行缓存清除，此时请求2查询到并使用了redis中的旧数据。
（2）第二种方案：当请求1执行清除缓存后，还未进行update操作，此时请求2进行查询到了旧数据并写入了redis。



所以：**先进行缓存清除，再执行update，最后（延迟N秒）再执行缓存清除。**

延迟N秒的时间要大于一次写redis操作的时间。这样做保证了一致性。以下图为例，线程2写入后被线程1删除。

![image_5.png](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bee4d1f2036e48d7926d6a299c9959c0~tplv-k3u1fbpfcp-watermark.image?)
